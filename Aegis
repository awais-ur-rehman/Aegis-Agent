{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":783889,"sourceType":"datasetVersion","datasetId":409297},{"sourceId":3783036,"sourceType":"datasetVersion","datasetId":2257980}],"dockerImageVersionId":31260,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport os\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler, StandardScaler\nfrom sklearn.metrics import accuracy_score, classification_report\nimport xgboost as xgb\nimport joblib\n\n# Set seeds for reproducibility\nSEED = 2026\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed(SEED)\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-07T15:49:15.314229Z","iopub.execute_input":"2026-02-07T15:49:15.314532Z","iopub.status.idle":"2026-02-07T15:49:22.298406Z","shell.execute_reply.started":"2026-02-07T15:49:15.314488Z","shell.execute_reply":"2026-02-07T15:49:22.297586Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"def train_voice_agent(file_path):\n    print(\"--- Training Voice Diagnostic Agent ---\")\n    \n    df = pd.read_csv(file_path)\n    \n    X = df.drop(['name', 'status'], axis=1)\n    y = df['status']\n    \n    # Split data\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=SEED)\n    \n    scaler = StandardScaler()\n    X_train_scaled = scaler.fit_transform(X_train)\n    X_test_scaled = scaler.transform(X_test)\n    \n    model = xgb.XGBClassifier(\n        n_estimators=100,\n        learning_rate=0.1,\n        max_depth=5,\n        use_label_encoder=False,\n        eval_metric='logloss'\n    )\n    \n    model.fit(X_train_scaled, y_train)\n    \n    preds = model.predict(X_test_scaled)\n    acc = accuracy_score(y_test, preds)\n    print(f\"Voice Agent Accuracy: {acc:.4f}\")\n    print(classification_report(y_test, preds))\n    \n    model.save_model('voice_agent_model.json')\n    joblib.dump(scaler, 'voice_agent_scaler.pkl')\n    print(\"Voice Agent artifacts saved: 'voice_agent_model.json' & 'voice_agent_scaler.pkl'\")\n\ntrain_voice_agent('/kaggle/input/parkinsons-data-set/parkinsons.data')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-07T15:49:22.300035Z","iopub.execute_input":"2026-02-07T15:49:22.300500Z","iopub.status.idle":"2026-02-07T15:49:22.663119Z","shell.execute_reply.started":"2026-02-07T15:49:22.300475Z","shell.execute_reply":"2026-02-07T15:49:22.661992Z"}},"outputs":[{"name":"stdout","text":"--- Training Voice Diagnostic Agent ---\nVoice Agent Accuracy: 0.9231\n              precision    recall  f1-score   support\n\n           0       0.90      0.82      0.86        11\n           1       0.93      0.96      0.95        28\n\n    accuracy                           0.92        39\n   macro avg       0.92      0.89      0.90        39\nweighted avg       0.92      0.92      0.92        39\n\nVoice Agent artifacts saved: 'voice_agent_model.json' & 'voice_agent_scaler.pkl'\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [15:49:22] WARNING: /workspace/src/learner.cc:790: \nParameters: { \"use_label_encoder\" } are not used.\n\n  bst.update(dtrain, iteration=i, fobj=obj)\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"class GaitDataset(Dataset):\n    def __init__(self, data_dir, sequence_length=128):\n        self.data = []\n        self.labels = []\n        self.sequence_length = sequence_length\n        \n        file_list = [f for f in os.listdir(data_dir) if f.endswith('.txt') and 'SHA' not in f and 'format' not in f and 'demographics' not in f]\n        \n        for file in file_list:\n            file_path = os.path.join(data_dir, file)\n            label = 1 if 'Pt' in file else 0 \n            \n            try:\n                curr_data = pd.read_csv(file_path, sep='\\t', header=None, on_bad_lines='skip')\n                sensor_data = curr_data.iloc[:, 1:].values.astype(np.float32)\n                \n                num_sequences = len(sensor_data) // sequence_length\n                for i in range(num_sequences):\n                    start = i * sequence_length\n                    end = start + sequence_length\n                    self.data.append(sensor_data[start:end])\n                    self.labels.append(label)\n                    \n            except Exception as e:\n                print(f\"Skipping {file}: {e}\")\n\n        self.data = np.array(self.data)\n        self.data = torch.tensor(self.data, dtype=torch.float32)\n        self.labels = torch.tensor(self.labels, dtype=torch.float32).unsqueeze(1)\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        return self.data[idx], self.labels[idx]\n\n# Configuration\nSEQ_LEN = 128\nBATCH_SIZE = 32\n\n# Load Data\nprint(\"--- Processing Gait Data ---\")\ngait_dir = '/kaggle/input/gait-in-parkinsons-disease'\nfull_dataset = GaitDataset(gait_dir, sequence_length=SEQ_LEN)\n\n# Split Train/Val\ntrain_size = int(0.8 * len(full_dataset))\ntest_size = len(full_dataset) - train_size\ntrain_dataset, test_dataset = torch.utils.data.random_split(full_dataset, [train_size, test_size])\n\ntrain_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\ntest_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n\nprint(f\"Total Sequences: {len(full_dataset)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-07T15:49:22.664015Z","iopub.execute_input":"2026-02-07T15:49:22.664247Z","iopub.status.idle":"2026-02-07T15:49:32.928890Z","shell.execute_reply.started":"2026-02-07T15:49:22.664226Z","shell.execute_reply":"2026-02-07T15:49:32.928062Z"}},"outputs":[{"name":"stdout","text":"--- Processing Gait Data ---\nTotal Sequences: 25730\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"class GaitCNNLSTM(nn.Module):\n    def __init__(self, input_dim, hidden_dim, num_layers, num_classes=1):\n        super(GaitCNNLSTM, self).__init__()\n        \n        self.cnn = nn.Sequential(\n            nn.Conv1d(in_channels=input_dim, out_channels=64, kernel_size=3, padding=1),\n            nn.ReLU(),\n            nn.BatchNorm1d(64),\n            nn.MaxPool1d(kernel_size=2),\n            nn.Dropout(0.3)\n        )\n        \n        self.lstm = nn.LSTM(\n            input_size=64, \n            hidden_size=hidden_dim, \n            num_layers=num_layers, \n            batch_first=True,\n            dropout=0.3\n        )\n        \n        self.fc = nn.Sequential(\n            nn.Linear(hidden_dim, 32),\n            nn.ReLU(),\n            nn.Linear(32, num_classes),\n            nn.Sigmoid()\n        )\n\n    def forward(self, x):\n        x = x.permute(0, 2, 1) \n        x = self.cnn(x) \n        x = x.permute(0, 2, 1)\n        \n        out, _ = self.lstm(x)\n        \n        out = out[:, -1, :] \n        \n        out = self.fc(out)\n        return out\n\nsample_data, _ = full_dataset[0]\nINPUT_DIM = sample_data.shape[1] \n\nmodel = GaitCNNLSTM(input_dim=INPUT_DIM, hidden_dim=128, num_layers=2).to(device)\ncriterion = nn.BCELoss()\noptimizer = optim.Adam(model.parameters(), lr=0.001)\n\nprint(f\"Model Initialized with Input Dimension: {INPUT_DIM}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-07T15:49:32.930140Z","iopub.execute_input":"2026-02-07T15:49:32.930794Z","iopub.status.idle":"2026-02-07T15:49:38.035453Z","shell.execute_reply.started":"2026-02-07T15:49:32.930770Z","shell.execute_reply":"2026-02-07T15:49:38.034814Z"}},"outputs":[{"name":"stdout","text":"Model Initialized with Input Dimension: 18\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"def train_gait_model(num_epochs=10):\n    print(\"--- Starting Gait Model Training ---\")\n    \n    for epoch in range(num_epochs):\n        model.train()\n        running_loss = 0.0\n        \n        for inputs, labels in train_loader:\n            inputs, labels = inputs.to(device), labels.to(device)\n            \n            optimizer.zero_grad()\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n            \n            running_loss += loss.item()\n            \n        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {running_loss/len(train_loader):.4f}\")\n        \n    # Evaluation\n    model.eval()\n    all_preds = []\n    all_labels = []\n    with torch.no_grad():\n        for inputs, labels in test_loader:\n            inputs = inputs.to(device)\n            outputs = model(inputs)\n            preds = (outputs > 0.5).float().cpu().numpy()\n            all_preds.extend(preds)\n            all_labels.extend(labels.numpy())\n            \n    print(\"Gait Model Accuracy:\", accuracy_score(all_labels, all_preds))\n    \n    # Save Model for Website\n    torch.save(model.state_dict(), 'gait_agent_weights.pth')\n    print(\"Gait Agent weights saved as 'gait_agent_weights.pth'\")\n\n# Run Training\ntrain_gait_model(num_epochs=15)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-07T15:49:38.037680Z","iopub.execute_input":"2026-02-07T15:49:38.038012Z","iopub.status.idle":"2026-02-07T15:50:16.706295Z","shell.execute_reply.started":"2026-02-07T15:49:38.037990Z","shell.execute_reply":"2026-02-07T15:50:16.705462Z"}},"outputs":[{"name":"stdout","text":"--- Starting Gait Model Training ---\nEpoch 1/15, Loss: 0.5399\nEpoch 2/15, Loss: 0.4667\nEpoch 3/15, Loss: 0.4111\nEpoch 4/15, Loss: 0.3271\nEpoch 5/15, Loss: 0.2798\nEpoch 6/15, Loss: 0.2148\nEpoch 7/15, Loss: 0.2028\nEpoch 8/15, Loss: 0.1549\nEpoch 9/15, Loss: 0.1242\nEpoch 10/15, Loss: 0.1106\nEpoch 11/15, Loss: 0.0978\nEpoch 12/15, Loss: 0.0896\nEpoch 13/15, Loss: 0.0788\nEpoch 14/15, Loss: 0.0705\nEpoch 15/15, Loss: 0.0586\nGait Model Accuracy: 0.9710454722114263\nGait Agent weights saved as 'gait_agent_weights.pth'\n","output_type":"stream"}],"execution_count":5},{"cell_type":"markdown","source":"# Orchestrate with LangGraph (The Multi-Agent Mesh)","metadata":{}},{"cell_type":"code","source":"!pip install langgraph langchain langchain_openai","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-07T15:50:16.707375Z","iopub.execute_input":"2026-02-07T15:50:16.707632Z","iopub.status.idle":"2026-02-07T15:50:36.327168Z","shell.execute_reply.started":"2026-02-07T15:50:16.707609Z","shell.execute_reply":"2026-02-07T15:50:36.326308Z"}},"outputs":[{"name":"stdout","text":"Collecting langgraph\n  Downloading langgraph-1.0.8-py3-none-any.whl.metadata (7.4 kB)\nRequirement already satisfied: langchain in /usr/local/lib/python3.12/dist-packages (0.3.27)\nCollecting langchain_openai\n  Downloading langchain_openai-1.1.7-py3-none-any.whl.metadata (2.6 kB)\nRequirement already satisfied: langchain-core>=0.1 in /usr/local/lib/python3.12/dist-packages (from langgraph) (0.3.79)\nCollecting langgraph-checkpoint<5.0.0,>=2.1.0 (from langgraph)\n  Downloading langgraph_checkpoint-4.0.0-py3-none-any.whl.metadata (4.9 kB)\nCollecting langgraph-prebuilt<1.1.0,>=1.0.7 (from langgraph)\n  Downloading langgraph_prebuilt-1.0.7-py3-none-any.whl.metadata (5.2 kB)\nCollecting langgraph-sdk<0.4.0,>=0.3.0 (from langgraph)\n  Downloading langgraph_sdk-0.3.4-py3-none-any.whl.metadata (1.6 kB)\nRequirement already satisfied: pydantic>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langgraph) (2.12.5)\nRequirement already satisfied: xxhash>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from langgraph) (3.6.0)\nRequirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.9 in /usr/local/lib/python3.12/dist-packages (from langchain) (0.3.11)\nRequirement already satisfied: langsmith>=0.1.17 in /usr/local/lib/python3.12/dist-packages (from langchain) (0.4.37)\nRequirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.12/dist-packages (from langchain) (2.0.44)\nRequirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.12/dist-packages (from langchain) (2.32.5)\nRequirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.12/dist-packages (from langchain) (6.0.3)\nINFO: pip is looking at multiple versions of langchain-openai to determine which version is compatible with other requirements. This could take a while.\nCollecting langchain_openai\n  Downloading langchain_openai-1.1.6-py3-none-any.whl.metadata (2.6 kB)\n  Downloading langchain_openai-1.1.5-py3-none-any.whl.metadata (2.6 kB)\n  Downloading langchain_openai-1.1.4-py3-none-any.whl.metadata (2.6 kB)\n  Downloading langchain_openai-1.1.3-py3-none-any.whl.metadata (2.6 kB)\n  Downloading langchain_openai-1.1.2-py3-none-any.whl.metadata (2.6 kB)\n  Downloading langchain_openai-1.1.1-py3-none-any.whl.metadata (2.6 kB)\n  Downloading langchain_openai-1.1.0-py3-none-any.whl.metadata (2.6 kB)\nINFO: pip is still looking at multiple versions of langchain-openai to determine which version is compatible with other requirements. This could take a while.\n  Downloading langchain_openai-1.0.3-py3-none-any.whl.metadata (2.6 kB)\n  Downloading langchain_openai-1.0.2-py3-none-any.whl.metadata (1.8 kB)\n  Downloading langchain_openai-1.0.1-py3-none-any.whl.metadata (1.8 kB)\n  Downloading langchain_openai-1.0.0-py3-none-any.whl.metadata (1.8 kB)\n  Downloading langchain_openai-0.3.35-py3-none-any.whl.metadata (2.4 kB)\nRequirement already satisfied: openai<3.0.0,>=1.104.2 in /usr/local/lib/python3.12/dist-packages (from langchain_openai) (2.15.0)\nRequirement already satisfied: tiktoken<1.0.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from langchain_openai) (0.12.0)\nRequirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core>=0.1->langgraph) (9.1.2)\nRequirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core>=0.1->langgraph) (1.33)\nRequirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core>=0.1->langgraph) (4.15.0)\nCollecting packaging<26.0.0,>=23.2.0 (from langchain-core>=0.1->langgraph)\n  Downloading packaging-25.0-py3-none-any.whl.metadata (3.3 kB)\nCollecting ormsgpack>=1.12.0 (from langgraph-checkpoint<5.0.0,>=2.1.0->langgraph)\n  Downloading ormsgpack-1.12.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.2 kB)\nINFO: pip is looking at multiple versions of langgraph-prebuilt to determine which version is compatible with other requirements. This could take a while.\nCollecting langgraph-checkpoint<5.0.0,>=2.1.0 (from langgraph)\n  Downloading langgraph_checkpoint-3.0.1-py3-none-any.whl.metadata (4.7 kB)\n  Downloading langgraph_checkpoint-3.0.0-py3-none-any.whl.metadata (4.2 kB)\n  Downloading langgraph_checkpoint-2.1.2-py3-none-any.whl.metadata (4.2 kB)\n  Downloading langgraph_checkpoint-2.1.1-py3-none-any.whl.metadata (4.2 kB)\n  Downloading langgraph_checkpoint-2.1.0-py3-none-any.whl.metadata (4.2 kB)\nCollecting langchain-text-splitters<1.0.0,>=0.3.9 (from langchain)\n  Downloading langchain_text_splitters-0.3.11-py3-none-any.whl.metadata (1.8 kB)\nINFO: pip is still looking at multiple versions of langgraph-prebuilt to determine which version is compatible with other requirements. This could take a while.\n  Downloading langchain_text_splitters-0.3.10-py3-none-any.whl.metadata (1.9 kB)\nCollecting pip>=25.2 (from langchain-text-splitters<1.0.0,>=0.3.9->langchain)\n  Downloading pip-26.0.1-py3-none-any.whl.metadata (4.7 kB)\nINFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\nCollecting langchain-text-splitters<1.0.0,>=0.3.9 (from langchain)\n  Downloading langchain_text_splitters-0.3.9-py3-none-any.whl.metadata (1.9 kB)\nINFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\nCollecting langchain_openai\n  Downloading langchain_openai-0.3.34-py3-none-any.whl.metadata (2.4 kB)\n  Downloading langchain_openai-0.3.33-py3-none-any.whl.metadata (2.4 kB)\nCollecting openai<2.0.0,>=1.104.2 (from langchain_openai)\n  Downloading openai-1.109.1-py3-none-any.whl.metadata (29 kB)\nCollecting langchain_openai\n  Downloading langchain_openai-0.3.32-py3-none-any.whl.metadata (2.4 kB)\n  Downloading langchain_openai-0.3.31-py3-none-any.whl.metadata (2.4 kB)\n  Downloading langchain_openai-0.3.30-py3-none-any.whl.metadata (2.4 kB)\n  Downloading langchain_openai-0.3.29-py3-none-any.whl.metadata (2.4 kB)\n  Downloading langchain_openai-0.3.28-py3-none-any.whl.metadata (2.3 kB)\n  Downloading langchain_openai-0.3.27-py3-none-any.whl.metadata (2.3 kB)\n  Downloading langchain_openai-0.3.26-py3-none-any.whl.metadata (2.3 kB)\n  Downloading langchain_openai-0.3.25-py3-none-any.whl.metadata (2.3 kB)\n  Downloading langchain_openai-0.3.24-py3-none-any.whl.metadata (2.3 kB)\n  Downloading langchain_openai-0.3.23-py3-none-any.whl.metadata (2.3 kB)\n  Downloading langchain_openai-0.3.22-py3-none-any.whl.metadata (2.3 kB)\n  Downloading langchain_openai-0.3.21-py3-none-any.whl.metadata (2.3 kB)\n  Downloading langchain_openai-0.3.20-py3-none-any.whl.metadata (2.3 kB)\n  Downloading langchain_openai-0.3.19-py3-none-any.whl.metadata (2.3 kB)\n  Downloading langchain_openai-0.3.18-py3-none-any.whl.metadata (2.3 kB)\n  Downloading langchain_openai-0.3.17-py3-none-any.whl.metadata (2.3 kB)\n  Downloading langchain_openai-0.3.16-py3-none-any.whl.metadata (2.3 kB)\n  Downloading langchain_openai-0.3.15-py3-none-any.whl.metadata (2.3 kB)\n  Downloading langchain_openai-0.3.14-py3-none-any.whl.metadata (2.3 kB)\n  Downloading langchain_openai-0.3.13-py3-none-any.whl.metadata (2.3 kB)\n  Downloading langchain_openai-0.3.12-py3-none-any.whl.metadata (2.3 kB)\n  Downloading langchain_openai-0.3.11-py3-none-any.whl.metadata (2.3 kB)\n  Downloading langchain_openai-0.3.10-py3-none-any.whl.metadata (2.3 kB)\n  Downloading langchain_openai-0.3.9-py3-none-any.whl.metadata (2.3 kB)\n  Downloading langchain_openai-0.3.8-py3-none-any.whl.metadata (2.3 kB)\n  Downloading langchain_openai-0.3.7-py3-none-any.whl.metadata (2.3 kB)\n  Downloading langchain_openai-0.3.6-py3-none-any.whl.metadata (2.3 kB)\n  Downloading langchain_openai-0.3.5-py3-none-any.whl.metadata (2.3 kB)\n  Downloading langchain_openai-0.3.4-py3-none-any.whl.metadata (2.3 kB)\n  Downloading langchain_openai-0.3.3-py3-none-any.whl.metadata (2.7 kB)\n  Downloading langchain_openai-0.3.2-py3-none-any.whl.metadata (2.7 kB)\n  Downloading langchain_openai-0.3.1-py3-none-any.whl.metadata (2.7 kB)\n  Downloading langchain_openai-0.3.0-py3-none-any.whl.metadata (2.7 kB)\n  Downloading langchain_openai-0.2.14-py3-none-any.whl.metadata (2.7 kB)\n  Downloading langchain_openai-0.2.13-py3-none-any.whl.metadata (2.7 kB)\n  Downloading langchain_openai-0.2.12-py3-none-any.whl.metadata (2.7 kB)\n  Downloading langchain_openai-0.2.11-py3-none-any.whl.metadata (2.7 kB)\n  Downloading langchain_openai-0.2.10-py3-none-any.whl.metadata (2.6 kB)\n  Downloading langchain_openai-0.2.9-py3-none-any.whl.metadata (2.6 kB)\n  Downloading langchain_openai-0.2.8-py3-none-any.whl.metadata (2.6 kB)\n  Downloading langchain_openai-0.2.7-py3-none-any.whl.metadata (2.6 kB)\n  Downloading langchain_openai-0.2.6-py3-none-any.whl.metadata (2.6 kB)\n  Downloading langchain_openai-0.2.5-py3-none-any.whl.metadata (2.6 kB)\n  Downloading langchain_openai-0.2.4-py3-none-any.whl.metadata (2.6 kB)\n  Downloading langchain_openai-0.2.3-py3-none-any.whl.metadata (2.6 kB)\n  Downloading langchain_openai-0.2.2-py3-none-any.whl.metadata (2.6 kB)\n  Downloading langchain_openai-0.2.1-py3-none-any.whl.metadata (2.6 kB)\n  Downloading langchain_openai-0.2.0-py3-none-any.whl.metadata (2.6 kB)\n  Downloading langchain_openai-0.1.25-py3-none-any.whl.metadata (2.6 kB)\n  Downloading langchain_openai-0.1.24-py3-none-any.whl.metadata (2.6 kB)\n  Downloading langchain_openai-0.1.23-py3-none-any.whl.metadata (2.6 kB)\n  Downloading langchain_openai-0.1.22-py3-none-any.whl.metadata (2.6 kB)\n  Downloading langchain_openai-0.1.20-py3-none-any.whl.metadata (2.6 kB)\n  Downloading langchain_openai-0.1.19-py3-none-any.whl.metadata (2.6 kB)\n  Downloading langchain_openai-0.1.17-py3-none-any.whl.metadata (2.5 kB)\n  Downloading langchain_openai-0.1.16-py3-none-any.whl.metadata (2.5 kB)\n  Downloading langchain_openai-0.1.15-py3-none-any.whl.metadata (2.5 kB)\n  Downloading langchain_openai-0.1.14-py3-none-any.whl.metadata (2.5 kB)\n  Downloading langchain_openai-0.1.13-py3-none-any.whl.metadata (2.5 kB)\n  Downloading langchain_openai-0.1.12-py3-none-any.whl.metadata (2.5 kB)\n  Downloading langchain_openai-0.1.11-py3-none-any.whl.metadata (2.5 kB)\n  Downloading langchain_openai-0.1.10-py3-none-any.whl.metadata (2.5 kB)\n  Downloading langchain_openai-0.1.9-py3-none-any.whl.metadata (2.5 kB)\n  Downloading langchain_openai-0.1.8-py3-none-any.whl.metadata (2.5 kB)\n  Downloading langchain_openai-0.1.7-py3-none-any.whl.metadata (2.5 kB)\n  Downloading langchain_openai-0.1.6-py3-none-any.whl.metadata (2.5 kB)\n  Downloading langchain_openai-0.1.5-py3-none-any.whl.metadata (2.5 kB)\n  Downloading langchain_openai-0.1.4-py3-none-any.whl.metadata (2.5 kB)\n  Downloading langchain_openai-0.1.3-py3-none-any.whl.metadata (2.5 kB)\n  Downloading langchain_openai-0.1.2-py3-none-any.whl.metadata (2.5 kB)\n  Downloading langchain_openai-0.1.1-py3-none-any.whl.metadata (2.5 kB)\n  Downloading langchain_openai-0.0.8-py3-none-any.whl.metadata (2.5 kB)\n  Downloading langchain_openai-0.0.7-py3-none-any.whl.metadata (2.5 kB)\n  Downloading langchain_openai-0.0.6-py3-none-any.whl.metadata (2.5 kB)\n  Downloading langchain_openai-0.0.5-py3-none-any.whl.metadata (2.5 kB)\n  Downloading langchain_openai-0.0.4-py3-none-any.whl.metadata (2.5 kB)\n  Downloading langchain_openai-0.0.3-py3-none-any.whl.metadata (2.5 kB)\n  Downloading langchain_openai-0.0.2.post1-py3-none-any.whl.metadata (2.4 kB)\n  Downloading langchain_openai-0.0.2-py3-none-any.whl.metadata (570 bytes)\nCollecting langchain\n  Downloading langchain-1.2.9-py3-none-any.whl.metadata (5.7 kB)\nCollecting langchain-core>=0.1 (from langgraph)\n  Downloading langchain_core-1.2.9-py3-none-any.whl.metadata (4.4 kB)\nRequirement already satisfied: packaging>=23.2.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core>=0.1->langgraph) (26.0rc2)\nCollecting uuid-utils<1.0,>=0.12.0 (from langchain-core>=0.1->langgraph)\n  Downloading uuid_utils-0.14.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\nRequirement already satisfied: httpx>=0.25.2 in /usr/local/lib/python3.12/dist-packages (from langgraph-sdk<0.4.0,>=0.3.0->langgraph) (0.28.1)\nRequirement already satisfied: orjson>=3.10.1 in /usr/local/lib/python3.12/dist-packages (from langgraph-sdk<0.4.0,>=0.3.0->langgraph) (3.11.3)\nRequirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.104.2->langchain_openai) (4.12.1)\nRequirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.104.2->langchain_openai) (1.9.0)\nRequirement already satisfied: jiter<1,>=0.10.0 in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.104.2->langchain_openai) (0.10.0)\nRequirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.104.2->langchain_openai) (1.3.1)\nRequirement already satisfied: tqdm>4 in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.104.2->langchain_openai) (4.67.1)\nRequirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.7.4->langgraph) (0.7.0)\nRequirement already satisfied: pydantic-core==2.41.5 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.7.4->langgraph) (2.41.5)\nRequirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.7.4->langgraph) (0.4.2)\nRequirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.12/dist-packages (from tiktoken<1.0.0,>=0.7.0->langchain_openai) (2025.11.3)\nRequirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.5.0->openai<3.0.0,>=1.104.2->langchain_openai) (3.11)\nRequirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx>=0.25.2->langgraph-sdk<0.4.0,>=0.3.0->langgraph) (2026.1.4)\nRequirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx>=0.25.2->langgraph-sdk<0.4.0,>=0.3.0->langgraph) (1.0.9)\nRequirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx>=0.25.2->langgraph-sdk<0.4.0,>=0.3.0->langgraph) (0.16.0)\nRequirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core>=0.1->langgraph) (3.0.0)\nRequirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.17->langchain) (1.0.0)\nRequirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.17->langchain) (0.25.0)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->langchain) (3.4.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->langchain) (2.6.3)\nDownloading langgraph-1.0.8-py3-none-any.whl (158 kB)\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m158.1/158.1 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading langchain-1.2.9-py3-none-any.whl (111 kB)\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m111.2/111.2 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading langchain_openai-1.1.7-py3-none-any.whl (84 kB)\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m84.8/84.8 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading langchain_core-1.2.9-py3-none-any.whl (496 kB)\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m496.3/496.3 kB\u001b[0m \u001b[31m18.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading langgraph_checkpoint-4.0.0-py3-none-any.whl (46 kB)\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m46.3/46.3 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading langgraph_prebuilt-1.0.7-py3-none-any.whl (35 kB)\nDownloading langgraph_sdk-0.3.4-py3-none-any.whl (67 kB)\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m67.8/67.8 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading ormsgpack-1.12.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m212.6/212.6 kB\u001b[0m \u001b[31m17.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading uuid_utils-0.14.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (341 kB)\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m342.0/342.0 kB\u001b[0m \u001b[31m24.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: uuid-utils, ormsgpack, langgraph-sdk, langchain-core, langgraph-checkpoint, langchain_openai, langgraph-prebuilt, langgraph, langchain\n  Attempting uninstall: langchain-core\n    Found existing installation: langchain-core 0.3.79\n    Uninstalling langchain-core-0.3.79:\n      Successfully uninstalled langchain-core-0.3.79\n  Attempting uninstall: langchain\n    Found existing installation: langchain 0.3.27\n    Uninstalling langchain-0.3.27:\n      Successfully uninstalled langchain-0.3.27\nSuccessfully installed langchain-1.2.9 langchain-core-1.2.9 langchain_openai-1.1.7 langgraph-1.0.8 langgraph-checkpoint-4.0.0 langgraph-prebuilt-1.0.7 langgraph-sdk-0.3.4 ormsgpack-1.12.2 uuid-utils-0.14.0\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"import os\nimport torch\nimport joblib\nimport numpy as np\nimport pandas as pd\nfrom typing import TypedDict, List, Annotated\nfrom langgraph.graph import StateGraph, END\nfrom langchain_core.messages import HumanMessage, SystemMessage\n\nvoice_model = joblib.load('voice_agent_model.pkl')\nvoice_scaler = joblib.load('voice_agent_scaler.pkl')\n\nclass GaitCNNLSTM(torch.nn.Module):\n    def __init__(self, input_dim, hidden_dim, num_layers, num_classes=1):\n        super(GaitCNNLSTM, self).__init__()\n        self.cnn = torch.nn.Sequential(\n            torch.nn.Conv1d(input_dim, 64, kernel_size=3, padding=1),\n            torch.nn.ReLU(),\n            torch.nn.BatchNorm1d(64),\n            torch.nn.MaxPool1d(kernel_size=2),\n            torch.nn.Dropout(0.3)\n        )\n        self.lstm = torch.nn.LSTM(64, hidden_dim, num_layers, batch_first=True, dropout=0.3)\n        self.fc = torch.nn.Sequential(\n            torch.nn.Linear(hidden_dim, 32),\n            torch.nn.ReLU(),\n            torch.nn.Linear(32, num_classes),\n            torch.nn.Sigmoid()\n        )\n    def forward(self, x):\n        x = x.permute(0, 2, 1)\n        x = self.cnn(x)\n        x = x.permute(0, 2, 1)\n        out, _ = self.lstm(x)\n        out = self.fc(out[:, -1, :])\n        return out\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ngait_model = GaitCNNLSTM(input_dim=18, hidden_dim=128, num_layers=2).to(device)\ngait_model.load_state_dict(torch.load('gait_agent_weights.pth'))\ngait_model.eval()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-07T15:50:36.328659Z","iopub.execute_input":"2026-02-07T15:50:36.329263Z","iopub.status.idle":"2026-02-07T15:50:37.923800Z","shell.execute_reply.started":"2026-02-07T15:50:36.329233Z","shell.execute_reply":"2026-02-07T15:50:37.923229Z"}},"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"GaitCNNLSTM(\n  (cnn): Sequential(\n    (0): Conv1d(18, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n    (1): ReLU()\n    (2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (3): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (4): Dropout(p=0.3, inplace=False)\n  )\n  (lstm): LSTM(64, 128, num_layers=2, batch_first=True, dropout=0.3)\n  (fc): Sequential(\n    (0): Linear(in_features=128, out_features=32, bias=True)\n    (1): ReLU()\n    (2): Linear(in_features=32, out_features=1, bias=True)\n    (3): Sigmoid()\n  )\n)"},"metadata":{}}],"execution_count":7},{"cell_type":"code","source":"class AgentState(TypedDict):\n    user_id: str\n    voice_data: List[float]\n    gait_data: List[float]\n    voice_risk: float\n    gait_risk: float\n    final_diagnosis: str\n    payout_triggered: bool\n    transaction_receipt: str\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-07T15:50:37.924715Z","iopub.execute_input":"2026-02-07T15:50:37.925015Z","iopub.status.idle":"2026-02-07T15:50:37.929036Z","shell.execute_reply.started":"2026-02-07T15:50:37.924984Z","shell.execute_reply":"2026-02-07T15:50:37.928392Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"def voice_diagnostic_node(state: AgentState):\n    print(\"--- üéôÔ∏è Agent 1: Voice Specialist Analyzing... ---\")\n\n    data = np.array(state['voice_data']).reshape(1, -1)\n    scaled_data = voice_scaler.transform(data)\n    \n    prob = voice_model.predict_proba(scaled_data)[0][1]\n    \n    return {\"voice_risk\": float(prob)}\n\ndef gait_verification_node(state: AgentState):\n    print(\"--- üö∂ Agent 2: Gait Specialist Verifying... ---\")\n    dummy_input = torch.randn(1, 128, 18).to(device) \n    \n    with torch.no_grad():\n        prob = gait_model(dummy_input).item()\n        \n    return {\"gait_risk\": float(prob)}\n\ndef supervisor_node(state: AgentState):\n    print(\"--- üß† Supervisor Agent: Synthesizing Results... ---\")\n    v_risk = state.get('voice_risk', 0.0)\n    g_risk = state.get('gait_risk', 0.0)\n    \n    if v_risk > 0.8 and g_risk > 0.8:\n        diagnosis = \"High Risk: Parkinson's Detected\"\n        trigger = True\n    elif v_risk > 0.5 or g_risk > 0.5:\n        diagnosis = \"Moderate Risk: Clinical Monitoring Recommended\"\n        trigger = False\n    else:\n        diagnosis = \"Healthy: No Anomalies Detected\"\n        trigger = False\n        \n    return {\"final_diagnosis\": diagnosis, \"payout_triggered\": trigger}\n\ndef settlement_node(state: AgentState):\n    print(\"--- üí∞ FinTech Agent: Processing Insurance Payout... ---\")\n    if state['payout_triggered']:\n        receipt = f\"TX_HASH: 0x{os.urandom(4).hex()}... | STATUS: CONFIRMED | AMT: 500 USDC\"\n    else:\n        receipt = \"N/A - Payout Condition Not Met\"\n        \n    return {\"transaction_receipt\": receipt}\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-07T15:50:37.930103Z","iopub.execute_input":"2026-02-07T15:50:37.930480Z","iopub.status.idle":"2026-02-07T15:50:37.945759Z","shell.execute_reply.started":"2026-02-07T15:50:37.930449Z","shell.execute_reply":"2026-02-07T15:50:37.944948Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"\nworkflow = StateGraph(AgentState)\n\nworkflow.add_node(\"voice_agent\", voice_diagnostic_node)\nworkflow.add_node(\"gait_agent\", gait_verification_node)\nworkflow.add_node(\"supervisor\", supervisor_node)\nworkflow.add_node(\"fintech_agent\", settlement_node)\n\nworkflow.set_entry_point(\"voice_agent\")\nworkflow.add_edge(\"voice_agent\", \"gait_agent\")\nworkflow.add_edge(\"gait_agent\", \"supervisor\")\n\nworkflow.add_edge(\"supervisor\", \"fintech_agent\")\nworkflow.add_edge(\"fintech_agent\", END)\n\napp = workflow.compile()\n\nprint(\"LangGraph Agent Mesh Compiled Successfully!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-07T15:50:37.946655Z","iopub.execute_input":"2026-02-07T15:50:37.946946Z","iopub.status.idle":"2026-02-07T15:50:37.965784Z","shell.execute_reply.started":"2026-02-07T15:50:37.946924Z","shell.execute_reply":"2026-02-07T15:50:37.965235Z"}},"outputs":[{"name":"stdout","text":"LangGraph Agent Mesh Compiled Successfully!\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"\ndummy_voice_features = np.random.rand(22).tolist()\ndummy_gait_sequence = np.random.rand(128*18).tolist()\n\ninitial_state = {\n    \"user_id\": \"Patient_001\",\n    \"voice_data\": dummy_voice_features,\n    \"gait_data\": dummy_gait_sequence\n}\n\n# Run the Graph\nresult = app.invoke(initial_state)\n\nprint(\"\\n--- FINAL REPORT ---\")\nprint(f\"Diagnosis: {result['final_diagnosis']}\")\nprint(f\"Voice Risk: {result['voice_risk']:.2f}\")\nprint(f\"Gait Risk: {result['gait_risk']:.2f}\")\nprint(f\"Blockchain Receipt: {result['transaction_receipt']}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-07T15:50:37.966550Z","iopub.execute_input":"2026-02-07T15:50:37.966881Z","iopub.status.idle":"2026-02-07T15:50:37.993599Z","shell.execute_reply.started":"2026-02-07T15:50:37.966849Z","shell.execute_reply":"2026-02-07T15:50:37.992895Z"}},"outputs":[{"name":"stdout","text":"--- üéôÔ∏è Agent 1: Voice Specialist Analyzing... ---\n--- üö∂ Agent 2: Gait Specialist Verifying... ---\n--- üß† Supervisor Agent: Synthesizing Results... ---\n--- üí∞ FinTech Agent: Processing Insurance Payout... ---\n\n--- FINAL REPORT ---\nDiagnosis: Moderate Risk: Clinical Monitoring Recommended\nVoice Risk: 0.89\nGait Risk: 0.00\nBlockchain Receipt: N/A - Payout Condition Not Met\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n  warnings.warn(\n","output_type":"stream"}],"execution_count":11}]}