{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom sklearn.preprocessing import MinMaxScaler, StandardScaler\nfrom imblearn.over_sampling import SMOTE\nfrom torch.utils.data import DataLoader, TensorDataset\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\nvoice_df = pd.read_csv('/kaggle/input/parkinsons-data-set/parkinsons.data')\n\n# Drop 'name' (non-numeric) and 'status' (target)\nX_voice = voice_df.drop(['name', 'status'], axis=1).values\ny_voice = voice_df['status'].values\n\n# Normalize using MinMax: $x_{norm} = \\frac{x - \\min(x)}{\\max(x) - \\min(x)}$\nscaler_voice = MinMaxScaler(feature_range=(-1, 1))\nX_voice_scaled = scaler_voice.fit_transform(X_voice)\n\n# Balance the classes using SMOTE\nsmote = SMOTE(random_state=42)\nX_voice_final, y_voice_final = smote.fit_resample(X_voice_scaled, y_voice)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-07T12:43:43.950267Z","iopub.execute_input":"2026-02-07T12:43:43.950550Z","iopub.status.idle":"2026-02-07T12:43:50.017124Z","shell.execute_reply.started":"2026-02-07T12:43:43.950514Z","shell.execute_reply":"2026-02-07T12:43:50.016307Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"import os\nimport glob\n\n# Define columns: 1=Time, 2-9=Left sensors, 10-17=Right sensors, 18-19=Total Force\ngait_cols = + [f'L{i}' for i in range(8)] + +\n\ndef load_all_gait_files(path_pattern):\n    all_windows =\n    all_labels =\n    \n    file_list = glob.glob(path_pattern)\n    print(f\"Found {len(file_list)} gait files. Processing...\")\n\n    for file_path in file_list:\n        # Determine label from filename: 'Co' = 0 (Healthy), 'Pt' = 1 (Parkinson's)\n        label = 1 if 'GaPt' in file_path else 0\n        \n        try:\n            df = pd.read_csv(file_path, sep='\\s+', header=None, names=gait_cols)\n            # Remove Time and normalize\n            features = df.drop(, axis=1).values\n            scaled_features = StandardScaler().fit_transform(features)\n            \n            # Create 5-second windows (250 timesteps)\n            windows = create_windows(scaled_features)\n            \n            all_windows.append(windows)\n            all_labels.extend([label] * len(windows))\n        except Exception as e:\n            print(f\"Skipping {file_path} due to error: {e}\")\n\n    return np.vstack(all_windows), np.array(all_labels)\n\n# Run the bulk loader\nX_gait_bulk, y_gait_bulk = load_all_gait_files('/kaggle/input/gait-in-parkinsons-disease/*.txt')\nprint(f\"Final Gait Dataset: {X_gait_bulk.shape} samples with {len(np.unique(y_gait_bulk))} classes.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-07T12:43:55.691801Z","iopub.execute_input":"2026-02-07T12:43:55.692126Z","iopub.status.idle":"2026-02-07T12:43:55.698788Z","shell.execute_reply.started":"2026-02-07T12:43:55.692098Z","shell.execute_reply":"2026-02-07T12:43:55.697819Z"}},"outputs":[{"traceback":["\u001b[0;36m  File \u001b[0;32m\"/tmp/ipykernel_55/2471570161.py\"\u001b[0;36m, line \u001b[0;32m5\u001b[0m\n\u001b[0;31m    gait_cols = + [f'L{i}' for i in range(8)] + +\u001b[0m\n\u001b[0m                                                 ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"],"ename":"SyntaxError","evalue":"invalid syntax (2471570161.py, line 5)","output_type":"error"}],"execution_count":2},{"cell_type":"code","source":"class AegisFusionModel(nn.Module):\n    def __init__(self, voice_dim=22, gait_dim=18):\n        super(AegisFusionModel, self).__init__()\n        \n        # Voice Encoder\n        self.voice_branch = nn.Sequential(\n            nn.Linear(voice_dim, 64),\n            nn.BatchNorm1d(64),\n            nn.ReLU(),\n            nn.Linear(64, 128)\n        )\n        \n        # Gait Encoder: CNN for spatial + BiLSTM for temporal\n        self.gait_cnn = nn.Conv1d(gait_dim, 64, kernel_size=3, padding=1)\n        self.gait_lstm = nn.LSTM(64, 64, num_layers=2, batch_first=True, bidirectional=True)\n        self.gait_fc = nn.Linear(128, 128)\n        \n        # Cross-Modal Fusion \n        self.attention = nn.MultiheadAttention(embed_dim=128, num_heads=4)\n        \n        self.classifier = nn.Sequential(\n            nn.Linear(128, 1),\n            nn.Sigmoid()\n        )\n\n    def forward(self, v_x=None, g_x=None):\n        v_feat = self.voice_branch(v_x) if v_x is not None else None\n        \n        g_feat = None\n        if g_x is not None:\n            g_x = self.gait_cnn(g_x.transpose(1, 2)).transpose(1, 2)\n            _, (hn, _) = self.gait_lstm(g_x)\n            g_feat = self.gait_fc(torch.cat((hn[-2,:,:], hn[-1,:,:]), dim=1))\n            \n        # Use available modality for the final prediction\n        out_feat = v_feat if v_feat is not None else g_feat\n        if v_feat is not None and g_feat is not None:\n            # Attend to both if both exist\n            attn_out, _ = self.attention(v_feat.unsqueeze(0), g_feat.unsqueeze(0), g_feat.unsqueeze(0))\n            out_feat = attn_out.squeeze(0)\n            \n        return self.classifier(out_feat)\n\nmodel = AegisFusionModel().to(device)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Create updated DataLoaders\nv_loader = DataLoader(TensorDataset(torch.FloatTensor(X_voice_final).to(device), \n                                    torch.FloatTensor(y_voice_final).to(device)), \n                      batch_size=32, shuffle=True)\n\ng_loader = DataLoader(TensorDataset(torch.FloatTensor(X_gait_bulk).to(device), \n                                    torch.FloatTensor(y_gait_bulk).to(device)), \n                      batch_size=32, shuffle=True)\n\ndef train_aegis_final(model, epochs=100):\n    optimizer = optim.AdamW(model.parameters(), lr=0.0005, weight_decay=1e-4)\n    criterion = nn.BCELoss()\n    \n    for epoch in range(epochs):\n        model.train()\n        total_loss = 0\n        \n        # Zip allows the model to see a voice batch and a gait batch in one step\n        for (v_batch_x, v_batch_y), (g_batch_x, g_batch_y) in zip(v_loader, g_loader):\n            optimizer.zero_grad()\n            \n            # Forward passes for both modalities\n            v_out = model(v_x=v_batch_x)\n            g_out = model(g_x=g_batch_x)\n            \n            loss = criterion(v_out, v_batch_y.unsqueeze(1)) + criterion(g_out, g_batch_y.unsqueeze(1))\n            loss.backward()\n            optimizer.step()\n            total_loss += loss.item()\n            \n        if epoch % 10 == 0:\n            print(f\"Epoch {epoch} | Aggregated Loss: {total_loss/len(v_loader):.4f}\")\n\ntrain_aegis_final(model)","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}