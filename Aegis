{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":783889,"sourceType":"datasetVersion","datasetId":409297},{"sourceId":3783036,"sourceType":"datasetVersion","datasetId":2257980}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\nfrom imblearn.over_sampling import SMOTE\nimport matplotlib.pyplot as plt\n\n# Device configuration (Kaggle T4 GPU)\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load Voice Data\nvoice_df = pd.read_csv('/kaggle/input/parkinsons-data-set/parkinsons.data')\nvoice_data = voice_df.drop(['name', 'status'], axis=1)\nvoice_labels = voice_df['status']\n\n# Scale voice features\nvoice_scaler = MinMaxScaler(feature_range=(-1, 1))\nvoice_scaled = voice_scaler.fit_transform(voice_data)\n\n# Handle class imbalance using SMOTE\nsmote = SMOTE(random_state=42)\nvoice_final, labels_final = smote.fit_resample(voice_scaled, voice_labels)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load Gait Data (GaCo01_01.txt)\n# Columns: 1=Time, 2-9=Left VGRF, 10-17=Right VGRF, 18-19=Total Force\ngait_columns = + [f'L{i}' for i in range(8)] + +\ngait_df = pd.read_csv('/kaggle/input/gait-in-parkinsons-disease/GaCo01_01.txt', sep='\\s+', header=None, names=gait_columns)\n\n# Drop Time and normalize VGRF signals\ngait_features = gait_df.drop(, axis=1).values\ngait_scaler = StandardScaler()\ngait_scaled = gait_scaler.fit_transform(gait_features)\n\n# Windowing function: Creates segments for the CNN-BiLSTM\ndef create_windows(data, window_size=250):\n    windows =\n    for i in range(0, len(data) - window_size, window_size // 2): # 50% overlap\n        windows.append(data[i:i+window_size])\n    return np.array(windows)\n\ngait_windows = create_windows(gait_scaled) # Shape: (NumWindows, 250, 18)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class AegisFusionModel(nn.Module):\n    def __init__(self, voice_dim=22, gait_dim=18):\n        super(AegisFusionModel, self).__init__()\n        \n        # Voice Encoder (Dense)\n        self.voice_encoder = nn.Sequential(\n            nn.Linear(voice_dim, 64),\n            nn.BatchNorm1d(64),\n            nn.ReLU(),\n            nn.Dropout(0.3),\n            nn.Linear(64, 128)\n        )\n        \n        # Gait Encoder (1D-CNN + BiLSTM)\n        self.gait_cnn = nn.Conv1d(gait_dim, 64, kernel_size=3, padding=1)\n        self.gait_lstm = nn.LSTM(64, 64, num_layers=2, batch_first=True, bidirectional=True)\n        self.gait_fc = nn.Linear(128, 128) # 64 * 2 for bidirectional\n        \n        # Fusion Layer (Cross-Attention)\n        self.attention = nn.MultiheadAttention(embed_dim=128, num_heads=4)\n        \n        # Classification Head\n        self.classifier = nn.Sequential(\n            nn.Linear(128, 64),\n            nn.ReLU(),\n            nn.Linear(64, 1),\n            nn.Sigmoid()\n        )\n\n    def forward(self, voice_x, gait_x):\n        # Process Voice\n        v_feat = self.voice_encoder(voice_x) # (Batch, 128)\n        \n        # Process Gait\n        g_x = gait_x.transpose(1, 2) # (Batch, Dim, Seq)\n        g_x = self.gait_cnn(g_x).transpose(1, 2) # (Batch, Seq, 64)\n        _, (hn, _) = self.gait_lstm(g_x)\n        g_feat = torch.cat((hn[-2,:,:], hn[-1,:,:]), dim=1) # (Batch, 128)\n        g_feat = self.gait_fc(g_feat)\n        \n        # Cross-Attention Fusion\n        # We treat voice features as the query and gait as key/value\n        combined_feat, _ = self.attention(v_feat.unsqueeze(0), g_feat.unsqueeze(0), g_feat.unsqueeze(0))\n        \n        return self.classifier(combined_feat.squeeze(0))\n\nmodel = AegisFusionModel().to(device)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"criterion = nn.BCELoss()\noptimizer = optim.AdamW(model.parameters(), lr=0.0005, weight_decay=1e-4)\n\n# Note: In a real training scenario, you would pair voice and gait data \n# by Subject ID. Since these are separate datasets, for this Step 1 prototype, \n# we train on available labels to build the diagnostic capability.\n\ndef train_step(voice_batch, gait_batch, label_batch):\n    model.train()\n    optimizer.zero_grad()\n    outputs = model(voice_batch, gait_batch)\n    loss = criterion(outputs, label_batch.unsqueeze(1))\n    loss.backward()\n    optimizer.step()\n    return loss.item()\n\nprint(\"Architecture Ready. Proceeding to train Biometric Diagnostic Agent...\")","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}